{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all libraries used in this notebook\n",
    "\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "import re\n",
    "import librosa\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import importlib\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = 3 # ID of the selected datasets, 1 - ethernet, 3 - hva 280, 5 - hva 630\n",
    "selected_model = \"ClickDetectorCNN_64_128_LW.py\"\n",
    "model_weights = \"hva280_det_model_run_1_ch1_64_ch2_128.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary functions from the folder \"05_Utilities\"\n",
    "\n",
    "cwd = str(Path.cwd()) ## current working directory, should be the path to \"01_Dataset\", in which this notebook is located\n",
    "parent_dir = str(Path(cwd).parent) ## parent dir of the current working directory, should be the project directory\n",
    "utilities_dir_full_path = os.path.join(parent_dir, \"05_Utilities\") ## full path to the \"05_Utilities\" folder\n",
    "\n",
    "# import moduls for audio file processing and mel-spectrogram plotting\n",
    "\n",
    "sys.path.append(utilities_dir_full_path) ## add the path of the \"05_Utilities\" folder to the sys.path list\n",
    "\n",
    "import audioProcessing ## modul for audio file processing and mel-spectrogram generation\n",
    "import spectrogramPlotting ## modul for mel-spectrogram visualization\n",
    "import sharedValues ## modul for shared variables between the classes\n",
    "\n",
    "importlib.reload(audioProcessing)\n",
    "importlib.reload(spectrogramPlotting)\n",
    "importlib.reload(sharedValues)\n",
    "\n",
    "# make instances of the classes in the modules\n",
    "#processAudio = processAudio()\n",
    "processAudio = audioProcessing.processAudio()\n",
    "spectrogramPlotter = spectrogramPlotting.spectrogramPlotter()\n",
    "sharedValuesConfig =sharedValues.sharedValuesConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mateseidl/Library/CloudStorage/OneDrive-SZTAKI/_SZTAKI/03_Masters_thesis/01_TUM_Masterarbeit_Ausarbeitung/07_Click_event_detection_software/01_Electrical_Connector_Click-Event_Detection_git_repo/03_Click_Detection_Model/01_modelArchitectures\n",
      "Model weights have been loaded\n",
      "model: ClickDetectorCNN(\n",
      "  (block_1): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (block_2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=65536, out_features=1, bias=True)\n",
      "    (2): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# import detection model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_architectures_dir = \"03_Click_Detection_Model/01_modelArchitectures\"\n",
    "model_architectures_dir_path = os.path.join(parent_dir, model_architectures_dir)\n",
    "print(model_architectures_dir_path)\n",
    "\n",
    "if os.path.exists(model_architectures_dir_path):\n",
    "    sys.path.append(model_architectures_dir_path)\n",
    "    model_module = importlib.import_module(selected_model[:-3])\n",
    "    #from ClickDetectorCNN_v1 import ClickDetectorCNN\n",
    "    ClickDetectorCNN = getattr(model_module, 'ClickDetectorCNN') #access the ClickDetectorCNN class\n",
    "    model = ClickDetectorCNN(input_channels=1, output_shape=1).to(device)\n",
    "else:\n",
    "    print(\"Model architectures directory does not exist\")\n",
    "\n",
    "model = ClickDetectorCNN(input_channels=1, output_shape=1).to(device)\n",
    "\n",
    "# loda model weights\n",
    "\n",
    "model_weights_dir = \"03_Click_Detection_Model/02_savedWeights\"\n",
    "model_weights_dir_path = os.path.join(parent_dir, model_weights_dir)\n",
    "\n",
    "\n",
    "model_weights_full_path = os.path.join(model_weights_dir_path, model_weights)\n",
    "\n",
    "if os.path.exists(model_weights_full_path):\n",
    "    model.load_state_dict(torch.load(model_weights_full_path, map_location=device, weights_only=True)) # load model weights, map location of weights to device\n",
    "    model.to(device)\n",
    "    print(\"Model weights have been loaded\")\n",
    "    print(f\"model: {model}\")\n",
    "else:\n",
    "    print(\"Model weights file does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['01_Ethernet', '02_Ethernet_Test', '03_HVA280', '04_HVA280_Test', '05_HVA630', '06_HVA630_Test', '07_Noise_Samples', 'voice_memo_loc_mac.txt']\n"
     ]
    }
   ],
   "source": [
    "# import data from selected dataset\n",
    "\n",
    "audio_datasets = []\n",
    "audio_datasets_main_dir_path = None\n",
    "\n",
    "def access_data_from_local_system():\n",
    "    global audio_datasets_main_dir_path, audio_datasets\n",
    "    cwd = str(Path.cwd())\n",
    "    parent_dir = os.path.dirname(cwd)\n",
    "    audio_datasets_main_dir = \"01_Dataset/01_audioDatasets\"\n",
    "    audio_datasets_main_dir_path = os.path.join(parent_dir, audio_datasets_main_dir)\n",
    "\n",
    "    if os.path.exists(audio_datasets_main_dir_path):\n",
    "        for i in os.listdir(audio_datasets_main_dir_path):\n",
    "            #if folder name does not start with a dot\n",
    "            if i[0] != \".\":\n",
    "                audio_datasets.append(i)\n",
    "        audio_datasets = sorted(audio_datasets)\n",
    "    else:\n",
    "        print(\"Audio dataset directory does not exist\")\n",
    "\n",
    "    return audio_datasets\n",
    "\n",
    "audio_datasets = access_data_from_local_system()\n",
    "\n",
    "print(audio_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HVA280_test_1_dataset.npz', 'HVA280_test_2_dataset.npz', 'HVA280_test_3_dataset.npz', 'HVA280_test_4_dataset.npz', 'HVA280_test_5_dataset.npz']\n"
     ]
    }
   ],
   "source": [
    "# import data from selected test directory\n",
    "\n",
    "long_window = False\n",
    "if dataset_id == 2:\n",
    "    long_window = True\n",
    "\n",
    "def sort_key_func(file_name):\n",
    "        numbers = re.findall(r'_(\\d+)', file_name)\n",
    "        if numbers:\n",
    "            return int(numbers[0])\n",
    "        return file_name\n",
    "\n",
    "loaded_spec_chunks = None\n",
    "loaded_spec_chunk_labels = None\n",
    "\n",
    "dataset_dir_path = os.path.join(audio_datasets_main_dir_path, audio_datasets[dataset_id])\n",
    "\n",
    "file_list = []\n",
    "for f in os.listdir(dataset_dir_path):\n",
    "    if f.endswith('.npz'):\n",
    "        file_list.append(f)\n",
    "\n",
    "test_files_sorted = sorted(file_list, key=sort_key_func)\n",
    "\n",
    "print(test_files_sorted[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(file_id):\n",
    "    file_fullpath = os.path.join(dataset_dir_path, test_files_sorted[file_id])\n",
    "\n",
    "    file_name = test_files_sorted[file_id]\n",
    "\n",
    "    data = np.load(file_fullpath)\n",
    "    loaded_spec_chunks = data['spec_chunks']\n",
    "    loaded_spec_chunk_labels = data['labels']\n",
    "\n",
    "    return loaded_spec_chunks, loaded_spec_chunk_labels, file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize spectrogram chunks\n",
    "\n",
    "def normalize_spectrogram_chunks(spec_chunks):\n",
    "    # global min and max values for dB range\n",
    "    global_min = -120\n",
    "    global_max = 0\n",
    "\n",
    "    # convert to numpy array\n",
    "    spec_chunks = np.array(spec_chunks)\n",
    "\n",
    "    # normalize to the range [0, 1]\n",
    "    normalized_spectrograms = (spec_chunks - global_min) / (global_max - global_min)\n",
    "\n",
    "    return normalized_spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(X, y):\n",
    "    X = torch.from_numpy(X).type(torch.float32).unsqueeze(1) # convert to torch and add channel dimension\n",
    "    y = torch.from_numpy(y).type(torch.float32)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(X, y):\n",
    "    BATCH_SIZE = len(X)\n",
    "    test_dataset = torch.utils.data.TensorDataset(X, y)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prediction(model, test_loader):\n",
    "    test_acc = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_true_labels = []\n",
    "\n",
    "    binary_threshold = 0.5\n",
    "\n",
    "    def accuracy_fn(y_true, y_pred):\n",
    "            binary_predictions = (y_pred > binary_threshold).float()\n",
    "            correct = torch.eq(y_true, binary_predictions).sum().item() # torch.eq() calculates where two tensors are equal\n",
    "            acc = (correct / len(y_pred)) * 100\n",
    "            return acc\n",
    "\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in test_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # forward pass\n",
    "            test_pred = model(X)\n",
    "            test_pred = torch.squeeze(test_pred)\n",
    "\n",
    "            # calculate accuracy\n",
    "            binary_predictions = (test_pred > binary_threshold).float()\n",
    "\n",
    "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred)\n",
    "\n",
    "            all_preds.extend(binary_predictions.tolist())\n",
    "            all_true_labels.extend(y.tolist())\n",
    "\n",
    "        # divide total accuracy by length of test dataloader (per batch)\n",
    "        test_acc /= len(test_loader)\n",
    "\n",
    "        test_acc_rounded = round(test_acc, 2)\n",
    "\n",
    "    true_positive_indiced = [i for i, (pred, true) in enumerate(zip(all_preds, all_true_labels)) if pred == 1.0 and true == 1.0]\n",
    "    false_positive_indices = [i for i, (pred, true) in enumerate(zip(all_preds, all_true_labels)) if pred == 1.0 and true == 0.0]\n",
    "\n",
    "    detection_result = None\n",
    "    first_TP_index = None\n",
    "    first_FP_index = None\n",
    "\n",
    "    # delta error\n",
    "    delta = 1\n",
    "\n",
    "    if len(true_positive_indiced) == 0 and len(false_positive_indices) == 0:\n",
    "        detection_result = \"Detection failed\"\n",
    "    elif len(true_positive_indiced) == 0 and len(false_positive_indices) > 0:\n",
    "        detection_result = \"Detection failed\"\n",
    "        first_FP_index = min(false_positive_indices)\n",
    "    elif len(true_positive_indiced) > 0 and len(false_positive_indices) > 0 and min(false_positive_indices) < min(true_positive_indiced)-delta:\n",
    "        detection_result = \"Detection failed\"\n",
    "        first_TP_index = min(true_positive_indiced)\n",
    "        first_FP_index = min(false_positive_indices)\n",
    "    elif len(true_positive_indiced) > 0 and len(false_positive_indices) > 0 and min(true_positive_indiced) < min(false_positive_indices)+delta:\n",
    "        detection_result = \"Detection successful\"\n",
    "        first_TP_index = min(true_positive_indiced)\n",
    "        first_FP_index = min(false_positive_indices)\n",
    "    else:\n",
    "        detection_result = \"Detection successful\"\n",
    "        first_TP_index = min(true_positive_indiced)\n",
    "\n",
    "\n",
    "    true_positives = len([i for i, (pred, true) in enumerate(zip(all_preds, all_true_labels)) if pred == 1.0 and true == 1.0])\n",
    "    true_negatives = len([i for i, (pred, true) in enumerate(zip(all_preds, all_true_labels)) if pred == 0.0 and true == 0.0])\n",
    "    false_positives = len([i for i, (pred, true) in enumerate(zip(all_preds, all_true_labels)) if pred == 1.0 and true == 0.0])\n",
    "    false_negatives = len([i for i, (pred, true) in enumerate(zip(all_preds, all_true_labels)) if pred == 0.0 and true == 1.0])\n",
    "\n",
    "    return test_acc_rounded, true_positives, true_negatives, false_positives, false_negatives, detection_result, first_TP_index, first_FP_index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "save_results_dir = \"01_testResults\"\n",
    "save_results_dir_path = os.path.join(cwd, save_results_dir)\n",
    "\n",
    "for file_id in range(len(test_files_sorted)):\n",
    "    loaded_spec_chunks, loaded_spec_chunk_labels, file_name = load_test_data(file_id)\n",
    "    normalized_spectrograms = normalize_spectrogram_chunks(loaded_spec_chunks)\n",
    "    X, y = to_tensor(normalized_spectrograms, loaded_spec_chunk_labels)\n",
    "    test_loader = create_data_loader(X, y)\n",
    "    test_acc, true_positives, true_negatives, false_positives, false_negatives, detection_result, first_TP_index, first_FP_index = run_prediction(model, test_loader)\n",
    "\n",
    "    results.append([file_name, test_acc, true_positives, true_negatives, false_positives, false_negatives, detection_result, first_TP_index, first_FP_index])\n",
    "\n",
    "csv_file_name = model_weights[:-3] + \"_test\" + \".csv\"\n",
    "\n",
    "csv_file_path = os.path.join(save_results_dir_path, csv_file_name)\n",
    "\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['File Name', 'Test Accuracy', 'True Positives', 'True Negatives', 'False Positives', 'False Negatives', 'Detection Result', 'First TP Index', 'First FP Index'])\n",
    "    writer.writerows(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clickSense",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
